# TensorFlow Lite Micro Mbed Project for Benchmarking Models

This repository is partially auto generated by Tensorflow ([Project Generation](#project-generation)),
and contains source, header and project files needed to build a single TensorFlow Lite Micro (TFLu) target
using the Mbed command line interface ([mbed-cli](https://github.com/ARMmbed/mbed-cli)).


This project is meant for testing and benchmarking different Tensorflow Lite models.
These models can differ in their architecture, but also by the applied optimizations techniques
(quantizations, pruning, distillation,).

Therefore this project allows to compare the performance of various models,
but also the performance on different microcontrollers (MCU) and their respective architecture.

For automatic comparing and deployment of different models the Jupyter notebook 
[LeNet5-on-MNIST](https://gitlab.ethz.ch/tec/research/tensorflow/projects/ma_leheim/lenet5-on-mnist)
is meant to be used in conjunction with this mbed project.

This project **is** using [cmsis-nn](https://github.com/ARM-software/CMSIS_5/tree/develop/CMSIS/NN)
which enables acceleration on integer inference computations.


## Usage

### Setup the mbed environment

To load the dependencies this code requires, run:

```bash
mbed config root .
mbed deploy
```


For the moment the following manual fixes are necessary
([Issue 1](https://github.com/tensorflow/tensorflow/issues/37730) and [Issue 2](https://github.com/ARMmbed/mbed-os/issues/12568)).

Just execute the script `patch-cmsis.sh`:

```bash
./patch-cmsis.sh

```


Now you should be able to compile:

```bash
mbed compile -m auto -t GCC_ARM
```

If this works, it will give you a .bin file that you can flash onto the device
you're targeting. For example, using a Discovery STM3246G board, you can deploy
it by copying the bin to the volume mounted as a USB drive, just by dragging
over the file.

For the NUCLEO-L496ZG the following will build, flash and connect to the serial interface:

```bash
mbed compile -m NUCLEO_L496ZG -t GCC_ARM

cp BUILD/NUCLEO_L496ZG/GCC_ARM/mbed.bin /Volumes/NODE_L496ZG

screen /dev/tty.X 9600
```


### Options for the compilations

#### Macros

A couple of different benchmarking features are implemented via macros. 
Macros were used to allow for maximal performance and reduce unnecessary functions calls during the runtime.

The following macros have been implemented to enable different types of benchmarking and debugging:

##### `INPUT_LENGTH=N`

Sets the length of NN input. This is important for filling the input tensor.


##### `INPUT_TYPE`

*not in use yet*


##### `OUTPUT_LENGTH=N`

This sets the length of NN output -- important for reading the output tensor.

##### `OUTPUT_TYPE`

*not in use yet*


##### `CYCLES`

This macro sets the unit of benchmarking to cycles which might allow for more granular precision.
The implementation is seen in `benchmark.cc`. 

Make sure you know the clock frequency of your MCU if you're interested in absolute numbers.

The default unit is microseconds (us).


##### `BENCHMARK_LAYERS`

Setting this macro enables the individual benchmark of single layers.
The benchmarking of a whole of batch of inference gets disabled.
The MCU will report the benchmarking results for each layers of the neural network.


##### `NO_REPORTING`


This macro disables the output of the predictions made by the NN and benchmarking result.


##### `NO_MANUAL_INPUT`

This macro disables the manual input of input data.
The inference will loop indefinitely with the provided input data in `constants.cc`.

##### `BAUDRATE=N`

Sets the baud rate of the UART interface. 
Plays a significant role for the duration of the verfication of the testset on the MCU itself.

##### `ENERGY_MEASUREMENT`

Disables LEDs which indicate the current status.
Necessary for not falsifying energy measurements.

Furthermore it enables toggling GPIOs for the current status of the inference.


| GPIO  	| Indicates      	|
|-------	|-----------------	|
| D0	 	| Inference Status 	|
| D1		| Layer Status     	|
| D2     	| Input Status     	|
| D3     	|  *not used yet* 	|

Pin names can be found under `mbed-os/targets/TARGET_STM/TARGET_STM32L4/TARGET_STM32L496xG/TARGET_NUCLEO_L496ZG/PinNames.h` - depending on the target board.

When `BENCHMARK_LAYERS` is also enabled the GPIO D1 gets triggered and a waiting time of 500ms is introduced between each layer.

---

## Background

### Sources Files

*todo*

### Benchmarking

*todo*

At the moment the benchmarking class is used in `main_functions.cc` to benchmark complete inferences.
But also included within the TFLu source in `micro_interpreter.cc` to benchmark the single layers.



### Versions

This project has been generated with TF v2.2.0
and is using mbed-os 5.15.3 (`mbed-os-5.15.3`).


### Communication

The communication with the host is done via UART. 
The mentioned Jupyter notebooks uses the `serial` class to read the benchmarking results.

Each message consists of two lines sent via the interface:

```
name of the message
<value>

```

e.g.

```
Inference time in us
53321
LAYER_1_CONV2D
323
```


You may need to adjust the buffer size of TX and RX within mbed-os:
`mbed-os/drivers/mbed_lib.json`.

### Project Generation

The template of this project has been generated by:
```bash
git clone https://github.com/tensorflow/tensorflow
cd tensorflow
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS="cmsis-nn" generate_hello_world_mbed_project
```


See
[tensorflow/lite/micro](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro)
for details on how projects like this can be generated from the main source tree.

### Performance

#### cmsis-nn
 
Uses [cmsis-nn](https://github.com/ARM-software/CMSIS_5/tree/develop/CMSIS/NN) and therefore significantly
accelerates inferences with integer computations.

#### Build Profile

The compilation flags can be adjusted within the mbed-os profiles in `mbed-os/tools/porfiles/`.
By default mbed-os uses the `develop.json`.

For using a different build profile add `--profile RELEASE` when compiling.


#### Floating Point Unit (FPU)

For experimental purpose it is interesting to turn the FPU on/off.
By default mbed enables the FPU if the given MCU architecture supports it.
To disable it adjust the Python script for your compiler. For GCC: 
`mbed-os/tools/toolchain/gcc.py`.

E.g. for Cortex-M4, there you will find in line 99 `if core == "Cortex-M4F":`.
Adjust this string and the FPU will be disabled.


#### Bare-metal

This project is running with the bare metal profile of mbed-os to keep the overhead minimal.
For this benchmarking project no OS resources are necessary. 
[More about mbed-os bare metal](https://os.mbed.com/docs/mbed-os/v5.15/reference/mbed-os-bare-metal.html).

## Bugs & Missing Features

### Missing features

- Dynamic input and output datatype depending on the model.


## Updating this project

There are two major components which can be updated (hopefully)
independent of each other: **mbed-os** and **Tensorflow Lite Micro**.


### Updating mbed-os
For updating mbed-os adjust the target of `mbed-os.lib`. 
Or navigate to the `mbed-os` folder and checkout a recent mbed version: `git checkout mbed-os-X.X.X`

This project only uses UART interface implementation of mbed-os.

### Updating Tensorflow

*not tested yet*

Run another project generation from the Tensorflow source. Then either copy the the `src/` folder to the just generated project -- 
or copy the `tensorflow` and `third_party` folder to this project.


## License

TensorFlow's code is covered by the Apache2 License included in the repository,
and third party dependencies are covered by their respective licenses, in the
third_party folder of this package.
